{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vueotfvh_BId"
      },
      "source": [
        "Each file contains the history of simulation of a Markov Chain.\n",
        "\n",
        "Assignment: Estimate the context trees through BIC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guD0Fi2HcV4c"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mdStpok3cPT9"
      },
      "outputs": [],
      "source": [
        "import itertools # tools for iteration\n",
        "from collections import Counter # count number of repetitions in list\n",
        "import copy\n",
        "import numpy as np\n",
        "import random\n",
        "import networkx as nx # graph utilities\n",
        "import math\n",
        "from datetime import datetime\n",
        "from pandas import DataFrame # pandas dataframe\n",
        "import os # read directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKEtVNVJs4mv"
      },
      "source": [
        "Clean directory and clone repository with data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoRQ8hC-VME6",
        "outputId": "8aaebfb4-f282-4fff-ab44-c456d404d989"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n",
            "Cloning into 'MAE4007tasks'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 81 (delta 3), reused 0 (delta 0), pack-reused 72\u001b[K\n",
            "Unpacking objects: 100% (81/81), 607.23 KiB | 2.17 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r sample_data\n",
        "!rm -r MAE4007tasks\n",
        "!git clone --branch contextTreeRevision https://github.com/LucasMSpereira/MAE4007tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6B7dbIIfQ1D"
      },
      "source": [
        "# Read and organize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7UTlgcHdd3R",
        "outputId": "5a95f4a1-aefc-487a-a5c6-1b75cf00fdda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fileName: OrderEst_ex1.txt\n",
            "size: 10000\n",
            "valueCount: {1: 4726, 2: 5274}\n",
            "uniqueValues: [1, 2]\n",
            "penalty: 0.01\n",
            "\n",
            "fileName: OrderEst_ex2.txt\n",
            "size: 10000\n",
            "valueCount: {2: 6327, 1: 3673}\n",
            "uniqueValues: [1, 2]\n",
            "penalty: 0.01\n",
            "\n",
            "fileName: OrderEst_ex3.txt\n",
            "size: 10000\n",
            "valueCount: {1: 3943, 2: 2989, 3: 3068}\n",
            "uniqueValues: [1, 2, 3]\n",
            "penalty: 0.0001\n",
            "\n",
            "fileName: 52845.txt\n",
            "size: 30000\n",
            "valueCount: {1: 13668, 3: 8441, 2: 7891}\n",
            "uniqueValues: [1, 2, 3]\n",
            "penalty: 3.3333333333333335e-05\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# directory with data\n",
        "dataDir = './MAE4007tasks/contextTreeData'\n",
        "# list of dictionaries, each referring to a file\n",
        "data = []\n",
        "# read each file and fill 'data'\n",
        "for fileName in os.listdir(dataDir + \"/txtData\"):\n",
        "  # dictionary for current file\n",
        "  currentData = {}\n",
        "  data.append(currentData)\n",
        "  # file name\n",
        "  data[-1][\"fileName\"] = fileName\n",
        "  # open, read and close file\n",
        "  with open(dataDir + '/txtData/' + fileName, encoding = \"utf-8\") as f:\n",
        "    fileData = list(map(int, f.read()))\n",
        "  # file data as list of integers\n",
        "  data[-1][\"content\"] = fileData\n",
        "  # size of sample\n",
        "  data[-1][\"size\"] = len(data[-1][\"content\"])\n",
        "  # count occurrences of each value in current file\n",
        "  data[-1][\"valueCount\"] = dict(Counter(data[-1][\"content\"]))\n",
        "  # unique values in current file\n",
        "  data[-1][\"uniqueValues\"] = list(data[-1][\"valueCount\"].keys())\n",
        "  data[-1][\"uniqueValues\"].sort()\n",
        "  data[-1][\"penalty\"] = data[-1][\"size\"] ** (-(len(data[-1][\"uniqueValues\"]) - 1) / 2)\n",
        "for d in data:\n",
        "  for f in d:\n",
        "    if f != \"content\":\n",
        "      print(f\"{f}: {d[f]}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_JGGHPoIWw9"
      },
      "source": [
        "# BIC for context trees\n",
        "\n",
        "1.   Begin with the full context tree\n",
        "2.   Starting from leaves:\n",
        "\n",
        "  2.1.   Define the V value of nodes. In the case of leaves, use the likelihood of its word. In nodes, use the greater value between the\n",
        "likelihood of its word, and the product of the V values of its immediate successors\n",
        "\n",
        "  2.2.   Define the binary $\\chi$ value of nodes. Assign 1 if: the word's length is less than the maximum (tree depth); and the product of the V values of its immediate successors is greater than the likelihood of the word of the node. Assign 0 otherwise.\n",
        "3.   Remove nodes (and respective branches) that are dependent on a node whose $\\chi = 0$.\n",
        "\n",
        "\\*Assignment limits chain orders to 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeel6ztLmthS"
      },
      "source": [
        "Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uSAFfz_YmthS"
      },
      "outputs": [],
      "source": [
        "# maximum size of tree\n",
        "treeOrder = 8\n",
        "\n",
        "def addNodeToTree(nodeID: int, word: tuple, _contextTree: nx.DiGraph, df: DataFrame, fileID: int) -> None:\n",
        "  \"\"\"\n",
        "  Include node in tree. Connect it to successors\n",
        "  formed by appending each symbol in alphabet, and\n",
        "  predecessor (word without it's last symbol)\n",
        "  \"\"\"\n",
        "  _contextTree.add_node(nodeID)\n",
        "  # connect node to neighbors.\n",
        "  if len(word) > 1: # if word of size at least 2\n",
        "    _contextTree.add_edge( # connect to predecessor\n",
        "      IDofWord(word[:-1], df), nodeID\n",
        "    )\n",
        "    # if word isn't at lowest level of tree\n",
        "    if len(word) < treeOrder:\n",
        "      # connect to successors formed by appending each symbol in alphabet\n",
        "      for state in data[fileID][\"uniqueValues\"]:\n",
        "        _contextTree.add_edge(\n",
        "          nodeID, IDofWord(word + (state,), df)\n",
        "        )\n",
        "  else: # if word of length 1\n",
        "    # connect to root\n",
        "    _contextTree.add_edge(-1, nodeID)\n",
        "    # connect to successors formed by appending each symbol in alphabet\n",
        "    for state in data[fileID][\"uniqueValues\"]:\n",
        "      _contextTree.add_edge(\n",
        "        nodeID, IDofWord(word + (state,), df)\n",
        "      )\n",
        "\n",
        "def calcKhi(word: tuple, fileID: int, _df: DataFrame, _tree: nx.DiGraph):\n",
        "  \"\"\"Determine Khi value for current node in context tree\"\"\"\n",
        "  # product of Vs of successors\n",
        "  Vprod = successorsVprod(_tree, word, _df)\n",
        "  if len(word) < treeOrder and Vprod > wordLikelihood(word, fileID, _df) * data[fileID][\"penalty\"]:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def calcV(word: tuple, _tree: nx.DiGraph, _df: DataFrame, fileID: int):\n",
        "  \"\"\"Determine V value for a node in the context tree\"\"\"\n",
        "  wl = wordLikelihood(word, fileID, _df)\n",
        "  # if node of word is a leaf\n",
        "  if isLeaf(_tree, IDofWord(word, _df)):\n",
        "    return wl * data[fileID][\"penalty\"]\n",
        "  else: # if not a leaf\n",
        "    return max(wl * data[fileID][\"penalty\"], successorsVprod(_tree, word, _df))\n",
        "\n",
        "def IDofWord(word: tuple, _occurDataFrame: DataFrame):\n",
        "  \"\"\"ID a word's node (same as row in dataframe)\"\"\"\n",
        "  return _occurDataFrame[_occurDataFrame.word == word].index[0]\n",
        "\n",
        "def isLeaf(tree: nx.DiGraph, nodeID: int):\n",
        "  \"\"\"Boolean indicating if node is a leaf\"\"\"\n",
        "  return len(list(tree.successors(nodeID))) == 0\n",
        "\n",
        "def successorsVprod(tree: nx.DiGraph, _word: tuple, df: DataFrame):\n",
        "  \"\"\"Product of V values of a node's immediate successors\"\"\"\n",
        "  successors = tree.successors(IDofWord(_word, df))\n",
        "  return math.prod([tree.nodes[node][\"V\"] for node in successors])\n",
        "\n",
        "def wordCount(occurDataFrame: DataFrame, word: tuple) -> int:\n",
        "  \"\"\"Number of occurrences of certain word\"\"\"\n",
        "  return occurDataFrame.at[\n",
        "    occurDataFrame[occurDataFrame.word == word].index[0],\n",
        "    \"occurrences\"\n",
        "  ]\n",
        "\n",
        "def wordLikelihood(_word: tuple, fileIndex: int, df: DataFrame):\n",
        "  \"\"\"Calculate likelihood of word\"\"\"\n",
        "  likelihood = 1.0\n",
        "  nCounts = wordCount(df, _word)\n",
        "  if nCounts == 0:\n",
        "    return 1\n",
        "  # iterate in alphabet\n",
        "  for symbol in data[fileIndex][\"uniqueValues\"]:\n",
        "    newWordCount = wordCount(df, _word + (symbol,))\n",
        "    likelihood *= (newWordCount / nCounts) ** newWordCount\n",
        "  return likelihood"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loop in files\n",
        "for f in range(len(data)):\n",
        "  print(f\"\"\"\\n******\\nFile {f + 1}/{len(data)}   Time: {datetime.now().strftime(\"%H:%M:%S\")}\\n******\"\"\")\n",
        "  # directed graph for context tree\n",
        "  data[f][\"contextTree\"] = nx.DiGraph()\n",
        "  contextTree = data[f][\"contextTree\"]\n",
        "  # all possible words with size up to treeOrder + 1\n",
        "  allWords = list(\n",
        "      itertools.chain.from_iterable(\n",
        "          list(\n",
        "              [\n",
        "                list(\n",
        "                    itertools.product(\n",
        "                        data[f][\"uniqueValues\"],\n",
        "                        repeat = order\n",
        "                    )\n",
        "                ) for order in range(1, treeOrder + 1 + 1)\n",
        "              ]\n",
        "          )\n",
        "      )\n",
        "  )\n",
        "  # dictionaries with words, their lengths, and number of occurrences\n",
        "  occurrenceCount = [{\"word\": word, \"occurrences\": 0, \"size\": len(word)} for word in allWords]\n",
        "  # organize in pandas dataframe for convenience\n",
        "  occurrenceCount = DataFrame(occurrenceCount)\n",
        "  # build basic tree topology\n",
        "  print(\"\\nBuilding tree topology...\")\n",
        "  for (id, word) in enumerate(allWords):\n",
        "    if len(word) > 8:\n",
        "      continue\n",
        "    # add current word to tree\n",
        "    addNodeToTree(id, word, contextTree, occurrenceCount, f)\n",
        "    # set node attributes\n",
        "    nx.set_node_attributes(contextTree,\n",
        "        {id:\n",
        "          {\n",
        "              \"word\": ''.join(map(str, word)),\n",
        "              \"occurrences\": 0, \"khi\": -1, \"V\": -1, \"likelihood\": -1\n",
        "          }\n",
        "        }\n",
        "    )\n",
        "  # count occurrences of all words\n",
        "  print(\"Counting occurrences of all words...\\n\")\n",
        "  for wordSize in range(1, treeOrder + 1): # iterate in possible sizes of words\n",
        "    print(f\"\"\"Word size: {wordSize}    Time: {datetime.now().strftime(\"%H:%M:%S\")}\"\"\")\n",
        "    for position in range(data[f][\"size\"] - wordSize + 1): # iterate in sequence\n",
        "      word = data[f][\"content\"][position : position + wordSize] # current word\n",
        "      wordID = IDofWord(tuple(word), occurrenceCount) # id of current word\n",
        "      # increment number of times this word has been seen\n",
        "      occurrenceCount.at[wordID, \"occurrences\"] += 1 # update DataFrame\n",
        "      contextTree.nodes[wordID][\"occurrences\"] += 1 # update tree\n",
        "  # determine V and khi values by iterating in possible sizes of\n",
        "  # words in reversed order, so the tree is updated from bottom to top\n",
        "  print(\"\\nDetermining V and khi values...\\n\")\n",
        "  for wordSize in range(treeOrder, 0, -1):\n",
        "    print(f\"\"\"Tree level: {wordSize}    Time: {datetime.now().strftime(\"%H:%M:%S\")}\"\"\")\n",
        "    # IDs of words of current size\n",
        "    wordIndices = list(np.where(np.array(list(occurrenceCount['size'] == wordSize)) == True)[0])\n",
        "    for id in wordIndices: # iterate in words of current size\n",
        "      currentWord = tuple(map(int, contextTree.nodes[id][\"word\"]))\n",
        "      # calculate likelihood\n",
        "      contextTree.nodes[id][\"likelihood\"] = wordLikelihood(currentWord, f, occurrenceCount)\n",
        "      # calculate V\n",
        "      contextTree.nodes[id][\"V\"] = calcV(currentWord, contextTree, occurrenceCount, f)\n",
        "      # calculate khi\n",
        "      contextTree.nodes[id][\"khi\"] = calcKhi(currentWord, f, occurrenceCount, contextTree)"
      ],
      "metadata": {
        "id": "e1z7YiueyQ-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa3089f-c490-4d2f-b0fd-de70624ff577"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "******\n",
            "File 1/4   Time: 19:19:44\n",
            "******\n",
            "\n",
            "Building tree topology...\n",
            "Counting occurrences of all words...\n",
            "\n",
            "Word size: 1    Time: 19:19:44\n",
            "Word size: 2    Time: 19:19:56\n",
            "Word size: 3    Time: 19:20:05\n",
            "Word size: 4    Time: 19:20:11\n",
            "Word size: 5    Time: 19:20:18\n",
            "Word size: 6    Time: 19:20:24\n",
            "Word size: 7    Time: 19:20:30\n",
            "Word size: 8    Time: 19:20:37\n",
            "\n",
            "Determining V and khi values...\n",
            "\n",
            "Tree level: 8    Time: 19:20:43\n",
            "Tree level: 7    Time: 19:20:44\n",
            "Tree level: 6    Time: 19:20:45\n",
            "Tree level: 5    Time: 19:20:46\n",
            "Tree level: 4    Time: 19:20:46\n",
            "Tree level: 3    Time: 19:20:46\n",
            "Tree level: 2    Time: 19:20:46\n",
            "Tree level: 1    Time: 19:20:46\n",
            "\n",
            "******\n",
            "File 2/4   Time: 19:20:46\n",
            "******\n",
            "\n",
            "Building tree topology...\n",
            "Counting occurrences of all words...\n",
            "\n",
            "Word size: 1    Time: 19:20:46\n",
            "Word size: 2    Time: 19:20:53\n",
            "Word size: 3    Time: 19:20:59\n",
            "Word size: 4    Time: 19:21:06\n",
            "Word size: 5    Time: 19:21:12\n",
            "Word size: 6    Time: 19:21:18\n",
            "Word size: 7    Time: 19:21:25\n",
            "Word size: 8    Time: 19:21:32\n",
            "\n",
            "Determining V and khi values...\n",
            "\n",
            "Tree level: 8    Time: 19:21:39\n",
            "Tree level: 7    Time: 19:21:40\n",
            "Tree level: 6    Time: 19:21:41\n",
            "Tree level: 5    Time: 19:21:41\n",
            "Tree level: 4    Time: 19:21:42\n",
            "Tree level: 3    Time: 19:21:42\n",
            "Tree level: 2    Time: 19:21:42\n",
            "Tree level: 1    Time: 19:21:42\n",
            "\n",
            "******\n",
            "File 3/4   Time: 19:21:42\n",
            "******\n",
            "\n",
            "Building tree topology...\n",
            "Counting occurrences of all words...\n",
            "\n",
            "Word size: 1    Time: 19:22:45\n",
            "Word size: 2    Time: 19:23:16\n",
            "Word size: 3    Time: 19:23:48\n",
            "Word size: 4    Time: 19:24:22\n",
            "Word size: 5    Time: 19:24:54\n",
            "Word size: 6    Time: 19:25:27\n",
            "Word size: 7    Time: 19:25:59\n",
            "Word size: 8    Time: 19:26:32\n",
            "\n",
            "Determining V and khi values...\n",
            "\n",
            "Tree level: 8    Time: 19:27:06\n",
            "Tree level: 7    Time: 19:29:26\n",
            "Tree level: 6    Time: 19:30:55\n",
            "Tree level: 5    Time: 19:31:28\n",
            "Tree level: 4    Time: 19:31:40\n",
            "Tree level: 3    Time: 19:31:43\n",
            "Tree level: 2    Time: 19:31:44\n",
            "Tree level: 1    Time: 19:31:45\n",
            "\n",
            "******\n",
            "File 4/4   Time: 19:31:45\n",
            "******\n",
            "\n",
            "Building tree topology...\n",
            "Counting occurrences of all words...\n",
            "\n",
            "Word size: 1    Time: 19:32:50\n",
            "Word size: 2    Time: 19:34:28\n",
            "Word size: 3    Time: 19:36:09\n",
            "Word size: 4    Time: 19:37:50\n",
            "Word size: 5    Time: 19:39:32\n",
            "Word size: 6    Time: 19:41:13\n",
            "Word size: 7    Time: 19:42:56\n",
            "Word size: 8    Time: 19:44:36\n",
            "\n",
            "Determining V and khi values...\n",
            "\n",
            "Tree level: 8    Time: 19:46:18\n",
            "Tree level: 7    Time: 19:49:05\n",
            "Tree level: 6    Time: 19:50:42\n",
            "Tree level: 5    Time: 19:51:17\n",
            "Tree level: 4    Time: 19:51:28\n",
            "Tree level: 3    Time: 19:51:32\n",
            "Tree level: 2    Time: 19:51:33\n",
            "Tree level: 1    Time: 19:51:34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prune trees"
      ],
      "metadata": {
        "id": "YX-13L-jjJfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for f in range(len(data)): # iterate in files\n",
        "  print(f\"File {f + 1}/{len(data)}\")\n",
        "  data[f][\"prunedContextTree\"] = nx.DiGraph()\n",
        "  contextTree = data[f][\"contextTree\"]\n",
        "  b = True # binary \"activating\"\n",
        "  sizeOfLastWord = 0\n",
        "  for nodeID in nx.dfs_preorder_nodes(contextTree, source = -1):\n",
        "    if nodeID == -1:\n",
        "      continue\n",
        "    if len(contextTree.nodes[nodeID][\"word\"]) <= sizeOfLastWord:\n",
        "      b = True\n",
        "    if not b:\n",
        "      continue\n",
        "    # add current word to tree\n",
        "    # if len()\n",
        "    addNodeToTree(nodeID, tuple(map(int, contextTree.nodes[nodeID][\"word\"])), data[f][\"prunedContextTree\"], occurrenceCount, f)\n",
        "    # set node attributes\n",
        "    nx.set_node_attributes(data[f][\"prunedContextTree\"], {nodeID: contextTree.nodes[nodeID]})\n",
        "    # data[f][\"prunedContextTree\"].nodes[nodeID] = contextTree.nodes[nodeID]\n",
        "    if contextTree.nodes[nodeID][\"khi\"] == 0 and len(contextTree.nodes[nodeID][\"word\"]) < treeOrder:\n",
        "      b = False\n",
        "      sizeOfLastWord = len(contextTree.nodes[nodeID][\"word\"])"
      ],
      "metadata": {
        "id": "4_hA7M_BCYTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4780def9-1507-4f5e-a808-9296d3433179"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 1/4\n",
            "File 2/4\n",
            "File 3/4\n",
            "File 4/4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process order\n",
        "\n",
        "Estimate process order by size of longest word in pruned trees."
      ],
      "metadata": {
        "id": "k6PVaCM3XBSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for f in range(len(data)):\n",
        "  prunedTree = data[f][\"prunedContextTree\"]\n",
        "  wordSizes = [] # sizes of words with khi = 0\n",
        "  # iterate in depth-first seach nodes\n",
        "  for node in nx.dfs_preorder_nodes(prunedTree, source = -1):\n",
        "    if node == -1:\n",
        "      continue\n",
        "    try:\n",
        "      if prunedTree.nodes[node][\"khi\"] == 0:\n",
        "        wordSizes.append(len(prunedTree.nodes[node][\"word\"]))\n",
        "    except:\n",
        "      continue\n",
        "  print(\"Estimated order of markov process in file {:} is {:n}\".format(data[f][\"fileName\"], max(wordSizes)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmwq6jC9v6tN",
        "outputId": "cd27e0a2-0229-4941-a391-1cbd5f7b8e76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated order of markov process in file OrderEst_ex1.txt is 8\n",
            "Estimated order of markov process in file OrderEst_ex2.txt is 8\n",
            "Estimated order of markov process in file OrderEst_ex3.txt is 1\n",
            "Estimated order of markov process in file 52845.txt is 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}